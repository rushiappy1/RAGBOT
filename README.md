RAG-based Insurance Policy Q&A System
Status: Production-ready
Architecture: Docker microservices
LLM: Qwen2.5-7B (via llama.cpp, GPU-accelerated)

A highâ€‘precision Retrievalâ€‘Augmented Generation (RAG) system for answering insurance policy questions from raw PDF documents. It uses Hybrid Retrieval (BM25 + vector search) and a Strict Extractive Synthesis protocol to drive hallucinations effectively to zero.

âš¡ Quick Start (Docker / Production Setup)
Prerequisites
Docker & Docker Compose

NVIDIA drivers + NVIDIA Container Toolkit (for GPU acceleration)

1. Clone & Setup
bash
git clone <repo_url>
cd rag-insurance-bot
2. Launch the Stack
This single command starts:

Llama.cpp inference server (Qwen2.5-7B)

FastAPI backend

Streamlit UI

PostgreSQL (with pgvector)

Redis

Ingestion worker

bash
docker-compose up -d --build
3. Access Services
User Interface (Streamlit):
http://localhost:8501

Backend API Docs (FastAPI / OpenAPI):
http://localhost:8000/docs

LLM Server (llama.cpp):
http://localhost:8080

4. Stop the System
bash
docker-compose down
ğŸ— System Architecture
The system is deployed as a small cluster of containers:

Service	Container	Description	Port
Inference	ai_engine	Llama.cpp server (Qwen2.5-7B, GPU)	8080
Backend API	ai_backend	FastAPI app (retrieval + orchestration)	8000
Frontend	ai_frontend	Streamlit chat interface	8501
Database	rag_db	PostgreSQL + pgvector extension	5432
Cache / Queue	redis	Redis for session state / lightweight queue	6379
Ingest Worker	ai_ingest	Watches PDFs, runs chunking + embedding	â€”
Data Flow (High-Level)
text
graph LR
    PDF[PDF Upload / Drop-In] -->|Watch| Ingest[Ingestion Worker]
    Ingest -->|Chunking + Embedding| PG[(Postgres + pgvector + BM25)]
    User[User Query] -->|HTTP| Backend[FastAPI Backend]
    Backend -->|Hybrid Search| PG
    Backend -->|Context + Question| LLM[Llama.cpp / Qwen2.5-7B]
    LLM -->|Strict Extractive Answer| Backend
    Backend -->|JSON Response| UI[Streamlit Chat UI]
ğŸ›  Manual Setup (Local Dev / Debug)
Use this path only if youâ€™re debugging individual Python scripts or developing outside Docker.

Prerequisites
Python 3.10+

Local PostgreSQL

cmake and build toolchain (for llama.cpp)

(Optional) CUDA for GPU build

1. Environment
bash
python -m venv .RAGBOT
source .RAGBOT/bin/activate
pip install -r requirements.txt
2. Database (Dev)
bash
# Only DB via docker-compose (dev override)
docker-compose -f docker-compose.dev.yml up -d db
3. Services (Three Terminals)
Tab 1 â€“ Llama Server

bash
cd scripts/llama.cpp
./build/bin/llama-server \
  -m /models/Qwen2.5-7B-Instruct-Q6_K.gguf \
  -ngl 33 \
  --port 8080
Tab 2 â€“ Backend API

bash
uvicorn api.server:app --host 0.0.0.0 --port 8000 --reload
Tab 3 â€“ Frontend

bash
streamlit run ui/streamlit_app.py --server.port 8501
Note: run_full_system.py is deprecated in favor of Docker Compose.

Key Technical Design
Zero-Hallucination Strategy
Strict System Prompt

The LLM is instructed to:

Use only retrieved excerpts.

Copy answers verbatim from context.

Respond with:

Answer: <text>

Sources: [doc_id | chunk_id], [...]

If the answer is not explicitly present, it must return:
"Not found in provided excerpts."

Post-Generation Validation

After the LLM responds, the backend:

Checks whether the predicted answer text is a substring (normalized) of at least one retrieved chunk.

If not, the answer is rejected and replaced with "Not found in provided excerpts."

Fallback

Prefer no answer over hallucinated answer in a regulated domain.

Retrieval Tuning (Week 2 â€œLockedâ€ Settings)
Hybrid Retrieval: BM25 + vector similarity

bm25_weight = 0.7 â€“ prioritize exact term matches (e.g., â€œSection 4.1â€, â€œNCBâ€, â€œIDVâ€).

vector_weight = 0.3 â€“ capture semantic similarity and paraphrases.

Reranking:

Topâ€‘10 chunks from hybrid retrieval.

Optional crossâ€‘encoder reranker narrows to Topâ€‘5 chunks for the LLM context window.

 Project Structure
text
â”œâ”€â”€ api/                  # FastAPI application (RAG orchestration, /ask endpoint)
â”œâ”€â”€ config/               # System constants & config
â”‚   â””â”€â”€ synthesis_config.py  # Strict RAG + LLM settings (locked)
â”œâ”€â”€ data/                 # Data volumes (mapped in Docker)
â”‚   â”œâ”€â”€ chunks/           # JSONL semantic chunks
â”‚   â”œâ”€â”€ models/           # GGUF model files (Qwen, etc.)
â”‚   â””â”€â”€ policies/         # Raw PDF policies
â”œâ”€â”€ scripts/              # ETL, ingestion, and utilities
â”‚   â”œâ”€â”€ auto_ingest.py    # Watchdog: new PDFs â†’ pipeline
â”‚   â”œâ”€â”€ pdf2md_and_semantic_chunks.py  # PDF â†’ Markdown â†’ semantic chunks
â”‚   â”œâ”€â”€ embed_all_chunks.py            # Chunk embedding generator
â”‚   â”œâ”€â”€ hybrid_retrieval.py            # Retrieval debugging / inspection
â”‚   â”œâ”€â”€ eval_run.py       # Evaluation suite (golden dataset)
â”‚   â””â”€â”€ llama.cpp/        # Llama.cpp (inference engine submodule)
â”œâ”€â”€ ui/                   # Streamlit chatbot frontend
â”œâ”€â”€ docker-compose.yml    # Production orchestration (all services)
â””â”€â”€ Dockerfile.*          # Individual service images
Testing & Evaluation
Golden Dataset Evaluation
Run inside the backend container or local virtualenv:

bash
python scripts/eval_run.py
This writes metrics such as P@1, MRR, and hallucination rate to:

text
eval/eval_results.json
Retrieval-Only Testing
To debug or tune the retriever:

bash
python scripts/hybrid_retrieval.py
This allows inspection of the topâ€‘k retrieved chunks and their hybrid scores for sample queries.
